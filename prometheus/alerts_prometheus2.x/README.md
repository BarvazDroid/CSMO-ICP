# Base set of alerts for ICP 2.1.0.2+

Alerts are defined in Prometheus 2.x format (IBM Cloud Private 2.1.0.2 or later). Most of the rules are based on the [Prometheus Operator](https://github.com/coreos/prometheus-operator) rules modified to work with IBM Cloud Private.

Use the following command to replace existing alert definitions in Prometheus with provided alert rules:

- ICP 2.1.0.2

```
kubectl replace -f alert-rules2102.yaml  
```

- ICP 2.1.0.3

```
kubectl replace -f alert-rules2103.yaml  
```
In order to add selected alert rules to existing configuration, edit alerting ConfigMap in `kube-system` namespace and copy selected rules into `data:` section.

- ICP 2.1.0.2

```
kubectl edit cm alert-rules -n kube-system
```
- ICP 2.1.0.3

```
kubectl edit cm monitoring-prometheus-alertrules -n kube-system
```


Example alert rule:

```
- alert: ICPPreditciveHostDiskSpace
  expr: predict_linear(node_filesystem_free{mountpoint="/"}[4h], 4 * 3600) < 0
  for: 30m
  labels:
    severity: warning
  annotations:
    description: 'Based on recent sampling, the disk is likely to will fill on volume
            {{ $labels.mountpoint }} within the next 4 hours for instace: {{ $labels.instance_id
            }} tagged as: {{ $labels.instance_name_tag }}'
    summary: Predictive Disk Space Utilisation Alert
```

       
Example configuration of Prometheus AlertManager listed below, modifies behaviour of ICPMonitoringHeartbeat rule, to send heartbeat events to NOI Message Bus probe every 10 minutes:

```
apiVersion: v1
data:
  alertmanager.yml: |-
    global:
    receivers:
      - name: default-receiver
        webhook_configs:
        - url: 'http://<msgbus_probe_ip>:<msgbus_probe_port>/probe/webhook/prometheus'
          send_resolved: true
    route:
      group_by: ['alertname','instance','kubernetes_namespace','pod','container']
      group_wait: 10s
      group_interval: 5m
      receiver: default-receiver
      repeat_interval: 3h
      routes:
      - receiver: default-receiver
        match:
          alertname: ICPMonitoringHeartbeat
        repeat_interval: 5m
```

Use the following command to modify AlertManager ConfigMap in ICP:

```
kubectl edit cm monitoring-prometheus-alertmanager -n kube-system
```

The table below lists proposed base set of alerts:





| Alert Name                              | Severity | Summary                                                   | Message                                                                                                                                                                                               | Action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 
|-----------------------------------------|----------|-----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| 
| ICPMonitoringTargetDown                 | warning  | Targets are down                                          | {{ $value }}% or more of {{ $labels.job }} targets are down.                                                                                                                                          | Verify which scraping targets are down. Check the following URL: `https://<icp_console>:8443/prometheus/targets` to identify which instances are not available for scraping (metrics polling). More information about scraping jobs and instances: [https://prometheus.io/docs/concepts/jobs_instances](https://prometheus.io/docs/concepts/jobs_instances)                                                                                                                                                                                                                                                                                                                                 | 
| ICPMonitoringHeartbeat                  | none     | Alerting Heartbeat                                        | This is a Heartbeat event meant to ensure that the entire Alerting pipeline is functional.                                                                                                            | This notification is excpected every 10 minutes. Alert should be raised by the Event Manager (like Netcool OMNIBus) if notification is missing. In that case (alert because of missing heartbeat notification), verify that Promethus and AlertManager pods are running, network connectivity within alerting pipeline `Prometheus->AlertManager->Netcool` is functional and if there were any changes in the Prometheus or Netcool configuration. Check logs of Prometheus and Alertmanager Pods using: `kubectl logs <prometheus_pod> -n kube-system`, `kubectl logs <prometheus-alertmanager_pod> -n kube-system`                          | 
| ICPTooManyOpenFile Descriptors          | critical | Too many open file descriptors for the monitoring target  | {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.                                                | This alert allows to identify which Pod instance is close to the open file descriptors limit. File descriptor exhaustion leads to an instance being unavailable, requiring a restart to function properly again. Depending on the root cause, the solution may be an increasing of file descriptor limit, increasing the number of instances (scale up), application code change etc.                                                                                                                                                                                                                                                 | 
| ICPFdExhaustionClose                    | warning  | File descriptors soon exhausted for the monitoring target | {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.instance }}) instance will exhaust in file/socket descriptors soon                                                           | This alert allows to identify (based on last 1 hour trend) which Pod may exceed file descriptors limit. File descriptor exhaustion leads to an instance being unavailable, requiring a restart to function properly again. Depending on the root cause, the solution may be an increasing of file descriptor limit, increasing the number of instances (scale up), application code change etc.                                                                                                                                                                                                                                       | 
| ICPFdExhaustionClose                    | critical | File descriptors soon exhausted for the monitoring target | {{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.instance }}) instance will exhaust in file/socket descriptors soon                                                           | This alert allows to identify (based on last 1 hour trend) which Pod may exceed file descriptors limit. File descriptor exhaustion leads to an instance being unavailable, requiring a restart to function properly again. Depending on the root cause, the solution may be an increasing of file descriptor limit, increasing the number of instances (scale up), application code change etc.                                                                                                                                                                                                                                       | 
| ICPPodFrequentlyRestarting              | warning  | Pod is restarting frequently                              | Pod {{$labels.namespaces}}/{{$labels.pod}} is was restarted {{$value}} times within the last hour                                                                                                     | A Pod is restarting several times an hour. Verify Pod logs using ICP Kibana console. If the Pod name is known, use the following search query in the Kibana console: `kubernetes.pod: <pod_name>` If Pod is a part of k8s Deployment you can also use ICP Management console `Workload->Deployment->Logs`                                                                                                                                                                                                                                                                                                                        | 
| ICPApiserverDown                        | critical | API server unreachable                                    | Prometheus failed to scrape API server(s) or all API servers have disappeared from service discovery.                                                                                                 | Prometheus failed to scrape the API server, or API server have disappeared from service discovery. Verify status of the k8s apiserver using the following instructions: [https://stackoverflow.com/a/48669203](https://stackoverflow.com/a/48669203). You can also use the following command locally on ICP master node to check if the apiserver properly exposes Prometheus metrics: `curl http://localhost:8888/metrics`. Check k8s apiserver logs in the ICP Kibana console using the following search query: `kubernetes.container_name: apiserver`                                                                                                                      | 
| ICPApiServerLatency                     | warning  | Kubernetes apiserver latency is high                      | 99th percentile Latency for {{ $labels.verb }} requests to the kube-apiserver is higher than 1s.                                                                                                      | Alert is raised when 1% of  the API requests to the k8s apiserver in the last 10 minutes took more than 1 second. Check the apiserver logs to identify slow requests.  Tip: Kubernets apiserver logs are ingested by the ICP ElasticStack. Annotate response time filed in the apiserver logs to easily search for slow apiserver requests ICP Kibana.                                                                                                                                                                                                                                                                                                                                                    | 
| ICPNodeNotReady                         | warning  | Node status is NotReady                                   | The Kubelet on {{ $labels.node }} has not checked in with the API or has set itself to NotReady for more than an hour                                                                                 | List the status of the nodes using: `kubectl get nodes` and `kubectl describe node <node_name>`. Look for the following sections: `conditions`, `capacity` and `allocatable`. Check the node resource utilization (cpu, memory, storage) for the problematic node on the ICP Grafana dashboard: `Kubernetes Cluster Monitoring`. Logon via ssh to the problematic node and check kubelet logs using: `journalctl -u kubelet`                                                                                                                                                                                                             | 
| ICPManyNodesNotReady                    | critical | Many Kubernetes nodes are Not Ready                       | {{ $value }} Kubernetes nodes (more than 10% are in the NotReady state).                                                                                                                              | More than 10% of the listed number of Kubernetes nodes are NotReady. List status of the nodes: `kubectl get nodes` and `kubectl describe node <node_name>`. Look for the following sections: `conditions`, `capacity` and `allocatable`. Check node resource utilization for the problematic node on the ICP Grafana dashboard: `Kubernetes Cluster Monitoring`. Logon via ssh to the problematic node and check kubelet logs using: `journalctl -u kubelet`.                                                                                                                                                                             | 
| ICPKubeletDown                          | warning  | Many Kubelets cannot be scraped                           | Prometheus failed to scrape {{ $value }}% of kubelets.                                                                                                                                                | Identify which kubelet cannot be scraped by Prometheus using the following PromQL: `up{job="kubernetes-nodes"}` in the Prometheus console `https://<icp_console>:8443/prometheus` or check the status of the Prometheus job: `kubernetes-nodes` using: `https://<icp_console>:8443/prometheus/targets`. Check the status of the kubelet service on the worker node using command `systemctl status kubelet`. Check kubelet logs using: `journalctl -u kubelet`.                                                                                                                                                                        | 
| ICPKubeletDown                          | critical | Many Kubelets cannot be scraped                           | Prometheus failed to scrape {{ $value }}% of kubelets or all Kubelets have disappeared from service discovery.                                                                                        | Identify which kubelect cannot be scraped by Prometheus using the following PromQL: `up{job="kubernetes-nodes"}` in the Prometheus console `https://<icp_console>:8443/prometheus` or check the status of the Prometheus job: `kubernetes-nodes` using: `https://<icp_console>:8443/prometheus/targets`. Check the status of the kubelet service on the worker node using command `systemctl status kubelet`. Check kubelet logs using: `journalctl -u kubelet`.                                                                                                                                                                         | 
| ICPKubeletTooManyPods                   | warning  | Kubelet is close to pod limit                             | Kubelet {{$labels.instance}} is running {{$value}} pods close to the limit of 110                                                                                                                     | Add worker nodes in order to distribute Pods in your ICP cluster or reduce the number of running pods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 
| ICPNodeExporterDown                     | warning  | node-exporter cannot be scraped                           | Prometheus could not scrape a node-exporter for more than 10m or node-exporters have disappeared from discovery.                                                                                      | Prometheus failed to scrape the `node-exporter` on one on more worker nodes, or node exporter have disappeared from service discovery. Verify status of the `monitoring-prometheus-nodeexporter` DaemontSet using `kubectl get ds monitoring-prometheus-nodeexporter -n kube-system`. Node exporter pod should run on every worker node. Check the `monitoring-prometheus-nodeexporter` DaemonSet on the ICP console: `https://<ICP_console>:8443/console/workloads/daemonsets/kube-system/monitoring-prometheus-nodeexporter`. Check logs of the problematic  node-exporter pod using `kubectl logs <node_exporter-pod-name> -n kube-system` | 
| ICPNodeOutOfDisk                        | critical | Node ran out of disk space.                               | {{ $labels.node }} has run out of disk space.                                                                                                                                                         | Kubelet reported that there node is running out of disk space. Check the disk space on the node reported in the alert. Alert based on the node condidtion: `OutOfDisk`. Verify with `kubectl describe node <node_ip>` section `Conditions`.                                                                                                                                                                                                                                                                                                                                                                                              | 
| ICPNodeMemoryPressure                   | warning  | Node is under memory pressure.                            | {{ $labels.node }} is under memory pressure.                                                                                                                                                          | Kubelet reported that the node is running out of memory. Check the available memory on the node instance reported in the alert message. Alert based on the node condidtion: `MemoryPressure`. Verify with `kubectl describe node <node_ip>` section `Conditions`.                                                                                                                                                                                                                                                                                                                                                                                      | 
| ICPNodeDiskPressure                     | warning  | Node is under disk pressure.                              | {{ $labels.node }} is under disk pressure.                                                                                                                                                            | Kubelet reported that the node is running out of memory. Check the available memory on the node instance reported in the alert message. Alert based on the node condidtion: `MemoryPressure`. Verify with `kubectl describe node <node_ip>` section `Conditions`.                                                                                                                                                                                                                                                                                                                                                                                           | 
| ICPHostCPUUtilisation                   | warning  | CPU Utilisation Alert                                     | High CPU utilisation detected for instance {{ $labels.instance_id }} tagged as: {{ $labels.instance_name_tag }} the utilisation is currently: {{ $value }}%                                           | High CPU utilization on the ICP cluster host. Use the ICP Grafana dashboard: `Kubernetes Cluster monitoring` to identify which process or Pod is responsible for high CPU utilization and how CPU utilization changed in time for host, Pods and containers.                                                                                                                                                                                                                                                                                                                                                                          | 
| ICPPreditciveHostDiskSpace              | warning  | Predictive Disk Space Utilisation Alert                   | Based on recent sampling the disk is likely to will fill on volume {{ $labels.mountpoint }} within the next 4 hours for instace: {{ $labels.instance_id }} tagged as: {{ $labels.instance_name_tag }} | If disks keep filling up at the current pace they will run out of free space within the next hours. Alert message identifies the node instance and the mount point. Use Linux system commands to identify growing directory/files.                                                                                                                                                                                                                                                                                                                                                                                                            | 
| ICPHostHighMemeoryLoad                  | warning  | Memory utilization Alert                                  | Memory of a host is almost full for instance {{ $labels.instance_id }}                                                                                                                                | High memory utilization on the ICP cluster host. Use ICP Grafana dashboard: `Kubernetes Cluster monitoring` to identify which process or Pod is responsible for high memory utilization and how memory utilization changed in time for host, Pods and containers.                                                                                                                                                                                                                                                                                                                                                                | 
| ICPNodeSwapUsage                        | warning  | {{$labels.instance}}: Swap usage detected                 | {{$labels.instance}}: Swap usage usage is above 75% (current value is: {{ $value }})                                                                                                                  | Swap useage is higher than 75% which means that host server needs more RAM, there are too many containers running on the node or there is a memory leak in the running code.                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 
| PrometheusConfigReload Failed           | warning  | Prometheus configuration reload has failed                | Reloading Prometheus' configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}.                                                                                                          | Check the Prometheus pod logs using `kubectl logs <prometheus_pod_name> -n kube-system`. Most probably there is a syntax error in the Prometheus ConfigMap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 
| PrometheusNotificationQueue RunningFull | warning  | Prometheus alert notification queue is running full       | Prometheus alert notification queue is running full for {{$labels.namespace}}/{{$labels.pod}}                                                                                                         | Prometheus is generating more alerts than it can send to Alertmanagers in time. Check the status of the Prometheus AlertManager Pod. Check the logs of the Prometheus Pod. Check recent Prometheus alert configuration changes.                                                                                                                                                                                                                                                                                                                                                                                                        | 
| PrometheusErrorSendingAlerts            | warning  | Errors while sending alert from Prometheus                | Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}                                                                            | Check the status of the AlertManager Pod. Verify configured AlertManagers: `https://<icp_console>:8443/prometheus/status`. Verify AlertManager status and configuration: `https://<icp_console>:8443/alertmanager/#/status`. Check the Prometheus pod logs using `kubectl logs <prometheus_pod_name> -n kube-system`.                                                                                                                                                                                                                                                                                                                         | 
| PrometheusErrorSendingAlerts            | critical | Errors while sending alert from Prometheus                | Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}                                                                            | Check the status of the AlertManager Pod. Verify configured AlertManagers: `https://<icp_console>:8443/prometheus/status`. Verify AlertManager status and configuration: `https://<icp_console>:8443/alertmanager/#/status`. Check the Prometheus pod logs using `kubectl logs <prometheus_pod_name> -n kube-system`.                                                                                                                                                                                                                                                                                                                         | 
| PrometheusNotConnectedTo Alertmanagers  | warning  | Prometheus is not connected to any Alertmanagers          | Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alertmanagers                                                                                                             | Check the status of the AlertManager Pod. Verify configured AlertManagers: `https://<icp_console>:8443/prometheus/status`. Verify AlertManager status and configuration: `https://<icp_console>:8443/alertmanager/#/status`. Check the Prometheus pod logs using `kubectl logs <prometheus_pod_name> -n kube-system`.                                                                                                                                                                                                                                                                                                                        | 
